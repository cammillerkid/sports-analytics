import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import lxml

from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import statsmodels.api as sm


## Load data
url = "https://www.basketball-reference.com/leagues/NBA_2023_per_game.html"

nba_data = pd.read_html(url, header=0)[0]
nba_data.head()

nba_data.columns

def drop_rows_containing_string(df, col, string_value):
    mask = df[col].str.contains(string_value)
    df = df[~mask]
    return df

def convert_to_float(df, cols):
    for col in cols:
        df[col] = df[col].astype(float)
    return df


def create_name_column(df, name_list, column_name):
    df[column_name] = 0
    df.loc[df['Player'].isin(name_list), column_name] = 1
    return df

def row_number_by_player(df, player_col, sort_col):
    # Sort the DataFrame by the player column and the sort column
    df_sorted = df.sort_values([player_col, sort_col])

    # Create a new column to hold the row numbers
    df_sorted['row_number'] = (df_sorted.groupby(player_col).cumcount() + 1)

    # Return the sorted DataFrame with the row numbers
    return df_sorted


nba_data = row_number_by_player(nba_data, 'Player','G')
nba_data = nba_data[nba_data['row_number']==1]


## run both functions

features = ['Age', 'G', 'MP', 'FG%',
       '3P', '3PA', '3P%', '2P%', 'eFG%', 'FTA', 'FT%',
       'TRB', 'AST', 'STL', 'BLK', 'TOV', 'PTS']

## remove rows that have string values likes PTS 
cleaned_nba_data = drop_rows_containing_string(nba_data, 'PTS','PTS')
## convert all object data types to float values
final = convert_to_float(cleaned_nba_data, features)
## validate all types are correct
print(final[features].dtypes)


## Build Kmeans Model

# Extract the relevant features
data = final[features]

data = data.fillna(0)

# # Normalize the data
scaler = MinMaxScaler()
data = pd.DataFrame(scaler.fit_transform(data))
data

# # Step 2: Determine the optimal number of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# Step 3: Run the k-means algorithm
kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=0)
pred_y = kmeans.fit_predict(data)

# Step 4: Analyze the results
cluster_labels = pd.DataFrame(pred_y, columns=['ClusterLabel'])
result_df = pd.concat([final, cluster_labels], axis=1)

# print the number of players in each cluster
print(result_df['ClusterLabel'].value_counts())


## drop nulls
result_df.dropna(inplace=True)

print(result_df.groupby('ClusterLabel')[features].mean())


What's the best way to predict an all-star?
1/ K-means Clustering
2/ Logistic Regression
3/ Hueristics [PPG]
Create a list of all-stars from 2023 roster!

actual_all_stars = ['LeBron James',
                  'Stephen Curry', 
                  'Kevin Durant', 
                  'Joel Embiid', 
                  'Luka Dončić', 
                  'Giannis Antetokounmpo', 
                  'Nikola Jokić', 
                  'Kyrie Irving', 
                  'James Harden', 
                  'Jayson Tatum', 
                  'Bradley Beal', 
                  'Donovan Mitchell',
                  'Bam Adebayo',
                  'Jaylen Brown',
                  'Tyrese Haliburton',
                  'Jrue Holiday',
                  'DeMar Derozan',
                  'Julius Randle',
                  'Pascal Siakam',
                  'Lauri Markkanen',
                  'Zion Williamson',
                  'Anthony Edwards',
                  "De'Aaron Fox",
                  'Paul George',
                  'Shai Gilgeous-Alexander',
                  'Jaren Jackson Jr.',
                  'Damian Lillard',
                  'Domantas Sabonis',
                  'Ja Morant']


result_df = create_name_column(result_df, actual_all_stars, 'All Star')


1/ K Means
If we create clusters based on the players in season statistics, we should be able to reasonably isolate high and low performers. The plot below shows us that cluster 2 would capture 10% of all named all-stars, but unfortunately we see a wide distribution of all-star presence in 3 out of the 4 clusters formed.


all_star_plt = result_df.groupby('ClusterLabel')['All Star'].mean().reset_index()
sns.barplot(data=all_star_plt, x='ClusterLabel',y='All Star')


2/ Logistic Regression
We can use a classification model to predict all-star status, based on having 2023 all-stars roster being annouced last week. We see a really strong r-squared , explaining 88% of the variance for the target ['all star']. If we go ahead and use the model to predict which players the model would classify as an all-star, we 9 players missed in total.
Predicated All Stars, but NOT actually: ['Anthony Davis']
Actual All Stars, but NOT predicted: ['Bradley Beal', 'Jaren Jackson Jr.', 'Zion Williamson', 'Domantas Sabonis', 'DeMar Derozan', 'Julius Randle', 'Jayson Tatum', 'Pascal Siakam']



# select the relevant columns for the model
target = 'All Star'

# fit a logistic regression model
model = sm.Logit(result_df[target], sm.add_constant(result_df[features]))
result = model.fit()

# print the summary of the model
print(result.summary())


# predict the probabilities of being an all-star
probas = result.predict(sm.add_constant(result_df[features]))

# set the threshold for being an all-star
threshold = 0.5

# which is a common threshold value used in binary classification problems. 
# This means that if the predicted probability of a player being an all-star is greater than or equal to 0.5, the model predicts that the player is an all-star,
# otherwise it predicts that the player is not an all-star.

# get the list of players who should have been all-stars
predicted_all_stars = result_df.loc[probas >= threshold, 'Player'].tolist()
print(predicted_all_stars)
print(len(predicted_all_stars))


def compare_lists(list1, list2):
    actual_not_predicted = list(set(list1) - set(list2))
    predicted_not_actual = list(set(list2) - set(list1))

    return predicted_not_actual, actual_not_predicted
    
predicted_not_actual, actual_not_predicted = compare_lists(actual_all_stars, predicted_all_stars)

print("Predicated All Stars, but NOT actually:", predicted_not_actual)
print("Actual All Stars, but NOT predicted:", actual_not_predicted)



3/ Hueristic
The league indexs a lot of points. The players who score the most , by definition are the most exciting and will likely recieve a lot of votes. Lastly, we can use a simple hueristic to infer for the future. Right now, 2 out of 3 all-stars selected were in the top 30 of scoring. If you were lazy, and wanted to just use PPG next year in predicting, it might be the easiest bet.

## validate that the top scorers have allstar tags
df_sorted = result_df.sort_values(by='PTS', ascending=False)
top_scorers = df_sorted.head(30)
top_scorers['All Star'].mean()
